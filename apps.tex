
\section{Applications}

The goal of the FOKS system is primarily: for a user or a group of users to
agree upon a sequence of cryptographic keys so they can perform authenticated,
end-to-end encryption of arbitrary data. As a secondary goal, the system
exposes a set of authorized signing keys to sign on behalf of the group,
so that changes can be properly attributed. From here, we can build
any number of applications. 

For instance, one can imagine an MLS system for group messaging~\cite{MLS} where
the chat keys are the cryptographic combination (via something like HMAC or
SHA3) of: (1) the root of the MLS ratchet tree; and (2) the most current PTKs
available for the FOKS group. In this way, one can simultaneously achieve
Signal-style forward secrecy and FOKS-style team and device management. 

For our first FOKS prototypes, we have focused instead of two important applications:
first, at the foundational level, a simple key-value store. Members of the team
can put and get key-value pairs to the FOKS server. Keys and values are encrypted
with authenticated encryption agains the team's PTKs. A second application, built
atop the first, is an end-to-end encrypted Git server, that is compatiable with
legacy Git clients. We describe them both below.

\subsection{The FOKS Key-Value Store}

The FOKS Key-value (KV) store is an end-to-end encrypted key-value store, with 
a hierchical namespace, local to each party on the system. One can store a value
to any key of the form `/a/b/c'; the value can be a few bytes long, or many gigabytes.
The system provides simple \textit{put} and \textit{get} operations, but also
operations on the namespace, like listing, moving, and deletion of directories. That is,
one can perform an operation like \texttt{mv /a/b /foo} in roughly O(1) time, without
individually mofidying each of the entries stored under \texttt{/a/b}. Symlinks are also 
allowed. Though the system has some important Unix-style file system behaviors,
it does not implement full POSIX~\cite{posix-2017} semantics.

When a user puts a key-value pair into the store, it does so on behalf of a party,
whether themselves personally, or a team. It encrypts the key and value with the latest
PUK or PTK for the acting party. Of course PTKs and PUKs can rotate after the put
happens, so when getting values out of the store, the user might perform a decryption
with an older PTK or PUK, depending on the circumstances. We discuss rotation in
more detail below as we describe the various operations of the system.

\subsubsection{Making a New Directory}

The steps for making a new directory for party $p$ operating at role $r$ are as follows:

\begin{enumerate}
    \item Pick a random 32-byte directory key seed $s$.
    \item Pick a random 16-byte directory ID $i$.
    \item Derive $k$ for application \texttt{KVStore} from $p$'s PTK (or PUK) for role $r$
    \item Encrypt $s$ with key $k$ and nonce $i$.
    \item Post the ciphertext and the the directory ID $i$ to the server.
\end{enumerate}
%
This process creates an new empty directory, floating more or less in space. To link this 
new directory, we first need to walk to the appropriate place in the key tree, and then
modify the parent ``directory entry'' or \textit{dirent} to point to this new directory.
See Section~\ref{sec:walk} for more details on the walking process, but for now, assume
we have found the appropriate parent directory $d$. The client gets the directory key
seeds for the parent directory $d$ either from the server or from the client's local cache,
The client then derivew to keys from this key: a 32-byte HMAC key, and a 32-byte box key.
The client can now form a \textit{dirent} with the fields: (1) the new directory ID $i$; (2)
the parent dirent $d$; (3) the MACs of of the name of directory (the last component of the path),
using the MAC key derived just above; (4) the encryption of this name, using the box key
derived just above; (5) the version number (which starts at 1); (6) the role required to
overwite this entry; and (7) a ``binding MAC'' of the above fields, using the HMAC
key derived for parent directory $d$. The client posts this dirent up the server, and now
the new directory is linked into place. Note, the client can race another client here,
in which case there will be conflict over the triple composed of $d$, the name MAC, and 
the version number. The client that loses the race should download the winning dirent
and potentially try again.

Note that the directory name is MAC'ed and encrypted (using authenticated-encryption) separately.
The fields serve two different purposes: the former allows for lookups, where a client knows
a path and want to discover which directory (or later, file) it points to; the latter allows
for listing, where a client wants to know what is in a directory.

\subsubsection{Walking the Namespace}
\label{sec:walk}

Most operations start with a ``walk'' of the namespace, from the root down to the desired node.
For a path like \texttt{/a/b/c} there are of course three directories along the way,
and three corresponding dirents.  Each party has a designated root directory ID, initialized
by the first user of the KV store for this party. The client can fetch this directory 
with a special RPC, which returns the directory ID and its boxed directory key seeds.
To walk down to \texttt{/a}, the client MAC's the name \texttt{a} with MAC keys 
derived from these seeds, and then queries the server for the dirent in \texttt{/}
with the computer MAC, at the greatest version. This dirent, if found, contains a
```binding MAC```, which cryptographically binds the dirents fields together, as MACed
by the directory's MAC key. The client verifies this MAC, and if it passes, then the next
step in the path is the ``value'' field of the dirent, which contains the directory ID
of \texttt{/a}. This process continues until the leaf is reached.

To reduce latency, clients make heavy use of local caching. As they walk, they write down
which dirents they passed through and which versinos of those dirents that they saw.
At the end of the process, the send up the whole path to the server, asking if any dirent
was stale. If so, the clients repeats parts of the walk to get fresh data. 

Note there is a special ``value'' field for ``tombstones,'' which signify that the 
file at the path was deleted. That value can later be reinsted if a subsequent version
of the dirent replaces the tombstone with a value that points to a directory or file.

A dishonest server can withhold fresh dirent versions, which might mask file
updates, deletions or creations. Future improvements to this design might
include a transparency tree akin to the existing Merkle Trees to force the
server to be honest. The throughput and performance of this tree is a challenge
that we so far have not tackled.

\subsubsection{Creating Small Files}

Creating a small file (one less than 2KB) works much like creating a directory. The client:
%
\begin{enumerate}
    \item Creates a new random 16-byte file ID. 
    \item The plaintext is padded to a power of two no less than 32, to hide the exact size of the data.
    \item Encrypts the file data with: the KVStore key derived from the 
         current PTK (or PUK) at the given role; and with the file ID as the nonce.
    \item Puts the ciphertext to the server
    \item Walks the file path down to the correct dirent
    \item And writes the dirent pointing to the new file ID.
\end{enumerate}
%
Small files are immutable. To make an edit to a small file, a client uploads a new 
small file and replaces the dirent to point to the new file ID.


\subsubsection{Creating Large Files}

Files bigger than 2KB get a different treatment: each gets its own 32-byte encryption
key, and they are chunked into 4MB chunks. When the put commences, the client
creates this random 32-byte file key, and encrypts it for the party's latest
PTK (or PUK). Each chunk gets encrypted with this key, and a nonce constructed 
from: (1) the file ID; (2) the chunk's offset; (3) an ``is-final'' flag;
and (4) the unique type ID for chunk nonces. These fields cannot all fit into
Salsa20's 24-byte nonce field, so they are hashed and truncated to 24 bytes.
This nonce selection here prevents an evil server from reordering chunks,
lying about the end-of-the-file, or mixing-and-matching chunks from different
files.

\subsubsection{Key Rotations}
\label{sec:kv:rotations}

Given the KV stores persistent data, and new clients and members might need to
access old data, there is only so much we can do to prevent data theft if a
client device is compromised.  Consider this example: Alice is a member of team
``Acme'' and writes a file \texttt{/foo}.  Call her device key $d$, her PUK
$u_0$ and the PTK $t_0$. The server has access to the sequence of encryptions,
starting from the $d$, through $u_0$, $t_0$ and the file key $f$., that secure the contents of
\texttt{/foo}. The server \textit{needs} access to these data to broker access
to Alice's other devices, and the other members of team Acme.
Years pass, and device $d$ is lost, and Alice revokes $d$ after
realizing this.  Along the way, $u_0$ has rotated to $u_6$, and the latest Acme
PTK is now $t_{100}$.  And let's say all along, the FOKS clients of ACME members
rotate the encryptions stored in the KV store to use the latest PTKs. An evil
server who recovers device $d$ can still acceess the file \texttt{/foo}, despite
all the well-intenioned rotations, because it kept the original encryption of
\texttt{/foo}, encrypted with the original keys, and disobeyed commands to throw
away the ciphertext. In other words, if the server doesn't cooperate, there is
only so much that can be done to protect old data after a device compromise.  Of
course, future data is safe in this scenario.

The Keybase system takes the stance that rotating old file system
data is not worth it, due to scenario described above. If we 
assume the server is semi-honest, that is, if it sometimes (or always!)
throws away old ciphertexts, then we still made
security gains by rotating file system data, even if the server later
becomes evil.

The FOKS KV Store leaves the door open to later perform these
reencryptions, though so far we have not implemented it.
Clients should periodically sweep over all keys and values,
reencrypt them for the latest PTKs (or PUKs), and instruct
the server to discard the old ciphertexts. Due to per-file
keys for large files, there is no need to download and reupload
large file data. Instead, it suffices to reencrypt the file key.

There is no limit to the number of active entries in a directory,
so we cannot assume a directory will be rotated all at once. Thus, 
we allow for two valid directory key seeds per directory. As the
reencryption process sweeps across the directory, more dirents
use the new key, and fewer use the old, until the old key can 
be retired.

\subsubsection{Garbage Collection}

In its current form, the FOKS KV store does not perform any garbage collection,
opting instead to keep prior versions of values available indefinitely. We plan
to implement deletion in future versions so users can reclaim quota. 

\subsection{The FOKS Git Server}

FOKS includes an application written atop the KV store, which implements an
end-to-end encrypted (and authenticated) Git Server. FOKS URLs for 
a user looks like this:

\begin{verbatim}
foks://acme.host/alice/tango-proj
\end{verbatim}

Where \texttt{acme.host} is the FOKS server, \texttt{alice} is the user's
username, and \texttt{tango-proj} is the name of the repository. And
for a team:

\begin{verbatim}
foks://acme.host/t:interns/whiskey-proj
\end{verbatim}

Where \texttt{t:interns} is the team name. Users of the system
clone \texttt{git clone} this URL and then push and pull as
they would a conventional Git server. Data is written through to
the FOKS server at \texttt{acme.host}, into its KV-store,
of course end-to-end encrypted and authenticated so that the server
cannot read or modify the data.

\subsubsection{The Git Remote Helper Protocol}

FOKS's git integration is built with the Git Remote Helper 
Protocol~\cite{git-remote-helper}.  This protocol allows Git to interact with
remote storage systems other than its native SSH and HTTP protocols. When Git
encounters a URL with a custom scheme like \texttt{foks://}, it looks for an
executable named \texttt{git-remote-foks} in the system path. Git then launches
this helper process and communicates with it using a simple text-based protocol.

The protocol consists of Git sending commands like \texttt{capabilities},
\texttt{list}, \texttt{push}, and \texttt{fetch}, and the helper responding
with success/failure and data. The helper translates between
Git's object model and whatever remote storage system it interfaces with.

The more helper must deeply understand Git's object model. The 
helper gives commands of the form:
%
\begin{verbatim}
    push refs/heads/main:refs/heads/main
\end{verbatim}
%
and then the helper must understand which objects the server already
has, and must push only the missing objects.

The FOKS Git server writes a git remote to the user's 
FOKS KV space, under the prefix \texttt{/app/git}. To describe
how this application works, we first must summarize how the Git
Remote Helper Protocol~\cite{git-remote-helper} works.